# ğŸš€ Comandos para Ejecutar el Pipeline CDC Completo

## ğŸ“‹ Lista de Comandos Paso a Paso - Dual Pipeline CDC

### **1. ğŸ Activar Entorno Virtual**
```bash
# Navegar al directorio del proyecto
cd /home/nicolas/Documents/uade/instashop

# Activar entorno virtual
source venv/bin/activate
```

### **2. ğŸ³ Levantar Docker (Todos los Servicios)**
```bash
# Levantar todos los contenedores
docker-compose up -d

# Verificar que todos los servicios estÃ©n corriendo
docker-compose ps
```

### **3. ğŸ”§ Configurar CDC (Change Data Capture)**
```bash
# Configurar PostgreSQL para CDC
python nico-scripts/init_postgres_cdc.py

# Desplegar conectores Debezium
python nico-scripts/setup_debezium.py
```

### **4. ğŸ“Š Generar Eventos en PostgreSQL**
```bash
# Generar datos iniciales
python fake-data.py

# Ejecutar generador de datos realistas (CDC automÃ¡tico)
python nico-scripts/realistic_data_generator.py
```

### **5. ğŸ—„ï¸ Pipeline HistÃ³rico: CDC â†’ Data Warehouse**
```bash
# En una nueva terminal (mantener activado el venv)
source venv/bin/activate

# Ejecutar consumer CDC â†’ DWH
python nico-scripts/cdc_dwh_consumer.py
```

### **6. âš¡ Pipeline Tiempo Real: CDC â†’ Spark â†’ Redis**
```bash
# En otra nueva terminal (mantener activado el venv)
source venv/bin/activate

# Ejecutar Spark Streaming CDC â†’ Redis
python nico-scripts/spark-streaming/cdc_spark_redis.py
```

### **7. ğŸ“Š Levantar Dashboard**
```bash
# En otra nueva terminal (mantener activado el venv)
source venv/bin/activate

# Ejecutar dashboard de Streamlit (CDC + Spark + Redis)
streamlit run nico-scripts/dashboards/realtime_spark_dashboard.py
```

---

## ğŸ”„ Orden Recomendado de EjecuciÃ³n - Dual Pipeline CDC

### **Terminal 1: Generador de Datos (CDC AutomÃ¡tico)**
```bash
cd /home/nicolas/Documents/uade/instashop
source venv/bin/activate
python nico-scripts/realistic_data_generator.py
```

### **Terminal 2: Pipeline HistÃ³rico (CDC â†’ DWH)**
```bash
cd /home/nicolas/Documents/uade/instashop
source venv/bin/activate
python nico-scripts/cdc_dwh_consumer.py
```

### **Terminal 3: Pipeline Tiempo Real (CDC â†’ Spark â†’ Redis)**
```bash
cd /home/nicolas/Documents/uade/instashop
source venv/bin/activate
python nico-scripts/spark-streaming/cdc_spark_redis.py
```

### **Terminal 4: Dashboard (CDC + Spark + Redis)**
```bash
cd /home/nicolas/Documents/uade/instashop
source venv/bin/activate
streamlit run nico-scripts/dashboards/realtime_spark_dashboard.py
```

---

## ğŸ¯ URLs Importantes

| Servicio | URL | DescripciÃ³n |
|----------|-----|-------------|
| **Dashboard** | http://localhost:8501 | Streamlit Analytics |
| **Spark UI** | http://localhost:8080 | Spark Web Interface |
| **Jupyter** | http://localhost:8888 | Jupyter Notebooks |

---

## ğŸ” Comandos de Monitoreo

### **Ver Logs de Docker**
```bash
# Ver logs de todos los servicios
docker-compose logs -f

# Ver logs de un servicio especÃ­fico
docker-compose logs -f kafka1
docker-compose logs -f spark
docker-compose logs -f dwh-db
```

### **Ver Estado de Contenedores**
```bash
# Estado de todos los contenedores
docker-compose ps

# Ver uso de recursos
docker stats
```

### **Verificar Conexiones**
```bash
# Verificar Kafka
docker exec -it kafka1 kafka-topics.sh --list --bootstrap-server localhost:9092

# Verificar PostgreSQL
docker exec -it postgres_db psql -U insta -d instashop -c "SELECT COUNT(*) FROM Transaction;"
```

---

## ğŸ› ï¸ Comandos de Troubleshooting

### **Reiniciar Servicios**
```bash
# Reiniciar un servicio especÃ­fico
docker-compose restart kafka1

# Reiniciar todos los servicios
docker-compose restart
```

### **Limpiar y Reiniciar**
```bash
# Detener todos los servicios
docker-compose down

# Limpiar volÃºmenes (Â¡CUIDADO! Borra datos)
docker-compose down -v

# Levantar de nuevo
docker-compose up -d
```

### **Ver Logs de Error**
```bash
# Ver logs de error de un servicio
docker-compose logs --tail=50 kafka1

# Ver logs en tiempo real
docker-compose logs -f --tail=100
```

---

## ğŸ“Š Dual Pipeline CDC Completo

```
PostgreSQL â”€â”€â†’ Debezium â”€â”€â†’ Kafka â”€â”€â†’ Python Consumer â”€â”€â†’ DWH â”€â”€â†’ Dashboard
    â†‘              â†‘           â†‘            â†‘              â†‘
realistic_data  CDC Engine  CDC Topics   cdc_dwh_    Streamlit
_generator.py   (Automatic)             consumer.py  Dashboard
                                    â†“
                               Spark Streaming
                               (cdc_spark_redis.py)
                                    â†“
                                  Redis
                               (Real-time Cache)
```

### ğŸ”„ Flujos de Datos

#### **Pipeline HistÃ³rico (DWH)**
```
PostgreSQL â†’ Debezium â†’ Kafka â†’ Python Consumer â†’ DWH PostgreSQL
```
- **Datos**: Eventos individuales preservados
- **PropÃ³sito**: AnÃ¡lisis histÃ³rico y reportes
- **Latencia**: Cerca de tiempo real

#### **Pipeline Tiempo Real (Redis)**
```
PostgreSQL â†’ Debezium â†’ Kafka â†’ Spark Streaming â†’ Redis â†’ Dashboard
```
- **Datos**: MÃ©tricas agregadas y calculadas
- **PropÃ³sito**: Dashboard interactivo
- **Latencia**: Sub-segundo

---

## âš¡ Comandos RÃ¡pidos

### **Ejecutar Dual Pipeline CDC en Secuencia**
```bash
# 1. Activar venv
source venv/bin/activate

# 2. Levantar Docker
docker-compose up -d

# 3. Esperar 30 segundos para que los servicios estÃ©n listos
sleep 30

# 4. Configurar CDC
python nico-scripts/init_postgres_cdc.py
python nico-scripts/setup_debezium.py

# 5. Generar datos iniciales
python fake-data.py

# 6. Ejecutar generador CDC (en background)
python nico-scripts/realistic_data_generator.py &

# 7. Ejecutar pipeline histÃ³rico (en background)
python nico-scripts/cdc_dwh_consumer.py &

# 8. Ejecutar pipeline tiempo real (en background)
python nico-scripts/spark-streaming/cdc_spark_redis.py &

# 9. Levantar dashboard
streamlit run nico-scripts/dashboards/realtime_spark_dashboard.py
```

---

## ğŸ‰ Â¡Dual Pipeline CDC Listo!

Con estos comandos tienes todo el sistema CDC funcionando:
- âœ… **CDC AutomÃ¡tico**: Debezium captura cambios de PostgreSQL
- âœ… **Pipeline HistÃ³rico**: CDC â†’ DWH para anÃ¡lisis histÃ³rico
- âœ… **Pipeline Tiempo Real**: CDC â†’ Spark â†’ Redis para dashboard
- âœ… **Dashboard Interactivo**: MÃ©tricas en tiempo real
- âœ… **Monitoreo Completo**: Ambos pipelines funcionando simultÃ¡neamente

### ğŸ¯ Beneficios del Dual Pipeline CDC
- **ğŸ“Š Tiempo Real**: Dashboard con mÃ©tricas actualizadas al instante
- **ğŸ—„ï¸ HistÃ³rico**: Datos preservados para anÃ¡lisis a largo plazo
- **âš¡ Escalabilidad**: SeparaciÃ³n de responsabilidades
- **ğŸ”„ Resiliencia**: Fallback entre sistemas
- **ğŸ¨ Flexibilidad**: Diferentes latencias para diferentes necesidades
