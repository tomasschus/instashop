# ğŸ¢ InstaShop Data Warehouse (DWH) - Diagrama Entidad-RelaciÃ³n

## ğŸ“Š DER del Data Warehouse - Dual Pipeline CDC

```mermaid
erDiagram
    %% ===========================================
    %% DATA WAREHOUSE: dwh_db (Puerto 5436)
    %% ===========================================
    
    RealtimeEvents {
        serial id PK
        varchar event_type
        timestamp timestamp
        integer customer_id
        varchar customer_name
        integer product_id
        varchar product_name
        varchar category
        decimal amount
        varchar session_id
        jsonb raw_data
        timestamp processed_at
        varchar source
    }
    
    CustomerSessions {
        varchar session_id PK
        bigint customer_id
        jsonb session_data
        varchar outcome
        timestamp timestamp
    }
    
    %% ===========================================
    %% RELACIONES DEL DWH
    %% ===========================================
    
    CustomerSessions ||--o{ RealtimeEvents : "generates"
```

## ğŸ—ï¸ Estructura Detallada del DWH

### **ğŸ“Š Tabla: RealtimeEvents**

```mermaid
erDiagram
    RealtimeEvents {
        serial id PK "Clave primaria auto-incremental"
        varchar event_type "Tipo de evento (transaction, user_behavior, etc.)"
        timestamp timestamp "Timestamp del evento original"
        integer customer_id "ID del cliente (opcional)"
        varchar customer_name "Nombre del cliente (opcional)"
        integer product_id "ID del producto (opcional)"
        varchar product_name "Nombre del producto (opcional)"
        varchar category "CategorÃ­a del producto (opcional)"
        decimal amount "Monto/valor del evento (opcional)"
        varchar session_id "ID de sesiÃ³n (opcional)"
        jsonb raw_data "Datos originales del evento en JSON"
        timestamp processed_at "Timestamp de procesamiento en DWH"
        varchar source "Fuente del evento (kafka_consumer, etc.)"
    }
```

### **ğŸ‘¥ Tabla: CustomerSessions**

```mermaid
erDiagram
    CustomerSessions {
        varchar session_id PK "ID Ãºnico de la sesiÃ³n"
        bigint customer_id "ID del cliente"
        jsonb session_data "Datos completos de la sesiÃ³n en JSON"
        varchar outcome "Resultado de la sesiÃ³n (conversion, cart_abandonment, bounce)"
        timestamp timestamp "Timestamp de la sesiÃ³n"
    }
```

## ğŸ“‹ DescripciÃ³n de Tablas del DWH

### **ğŸ”„ RealtimeEvents - Eventos en Tiempo Real**

| Campo | Tipo | DescripciÃ³n | Ejemplo |
|-------|------|-------------|---------|
| **id** | SERIAL | Clave primaria auto-incremental | 1, 2, 3... |
| **event_type** | VARCHAR(50) | Tipo de evento procesado | `transaction`, `user_behavior`, `search`, `cart_event` |
| **timestamp** | TIMESTAMP | Fecha y hora del evento original | `2025-09-16 22:50:00` |
| **customer_id** | INTEGER | ID del cliente (opcional) | 28, 45, 67 |
| **customer_name** | VARCHAR(255) | Nombre del cliente (opcional) | `John Doe`, `Jane Smith` |
| **product_id** | INTEGER | ID del producto (opcional) | 15, 23, 89 |
| **product_name** | VARCHAR(255) | Nombre del producto (opcional) | `iPhone 15`, `Laptop Gaming` |
| **category** | VARCHAR(100) | CategorÃ­a del producto (opcional) | `Electronics`, `Clothing`, `Books` |
| **amount** | DECIMAL(12,2) | Monto/valor del evento (opcional) | 299.99, 150.50 |
| **session_id** | VARCHAR(255) | ID de sesiÃ³n del usuario (opcional) | `sess_123`, `sess_456` |
| **raw_data** | JSONB | Datos originales del evento | `{"items": [...], "payment_method": "..."}` |
| **processed_at** | TIMESTAMP | CuÃ¡ndo se procesÃ³ en el DWH | `2025-09-16 22:50:01` |
| **source** | VARCHAR(50) | Fuente del evento | `kafka_consumer`, `direct_insert` |

### **ğŸ‘¥ CustomerSessions - Sesiones de Clientes**

| Campo | Tipo | DescripciÃ³n | Ejemplo |
|-------|------|-------------|---------|
| **session_id** | VARCHAR(100) | ID Ãºnico de la sesiÃ³n | `sess_123`, `sess_456` |
| **customer_id** | BIGINT | ID del cliente | 28, 45, 67 |
| **session_data** | JSONB | Datos completos de la sesiÃ³n | `{"start_time": "...", "pages": [...], "duration": 300}` |
| **outcome** | VARCHAR(50) | Resultado de la sesiÃ³n | `conversion`, `cart_abandonment`, `bounce` |
| **timestamp** | TIMESTAMP | Timestamp de la sesiÃ³n | `2025-09-16 22:50:00` |

## ğŸ”„ Flujo de Datos hacia el DWH

### **1. ğŸ“¥ Entrada desde Kafka Consumer**

```mermaid
graph LR
    A[Kafka Topics] --> B[Consumer]
    B --> C[RealtimeEvents]
    B --> D[Transaction Items]
    D --> E[Kafka para Spark]
```

### **2. ğŸ“Š Tipos de Eventos Procesados**

| Evento | DescripciÃ³n | Campos Populados |
|--------|-------------|------------------|
| **transaction** | Evento principal de transacciÃ³n | `customer_id`, `customer_name`, `amount`, `raw_data` |
| **transaction_item** | Item individual de transacciÃ³n | `product_id`, `product_name`, `category`, `amount` |
| **user_behavior** | Comportamiento del usuario | `customer_id`, `session_id`, `raw_data` |
| **search** | BÃºsqueda realizada | `customer_id`, `session_id`, `raw_data` |
| **cart_event** | Evento de carrito | `customer_id`, `amount`, `session_id`, `raw_data` |

### **3. ğŸ¯ Agregaciones y AnÃ¡lisis**

```mermaid
graph TB
    A[RealtimeEvents] --> B[AnÃ¡lisis por Evento]
    A --> C[AnÃ¡lisis por Cliente]
    A --> D[AnÃ¡lisis por Producto]
    A --> E[AnÃ¡lisis por CategorÃ­a]
    A --> F[AnÃ¡lisis Temporal]
    
    B --> G[MÃ©tricas de Comportamiento]
    C --> H[Perfil de Cliente]
    D --> I[Performance de Productos]
    E --> J[Tendencias por CategorÃ­a]
    F --> K[Patrones Temporales]
```

## ğŸ” Consultas TÃ­picas del DWH

### **ğŸ“Š MÃ©tricas de Transacciones**
```sql
SELECT 
    DATE(timestamp) as fecha,
    COUNT(*) as total_transacciones,
    SUM(amount) as ingresos_totales,
    AVG(amount) as ticket_promedio
FROM realtime_events 
WHERE event_type = 'transaction'
GROUP BY DATE(timestamp)
ORDER BY fecha DESC;
```

### **ğŸ‘¥ AnÃ¡lisis de Clientes**
```sql
SELECT 
    customer_id,
    customer_name,
    COUNT(*) as total_eventos,
    COUNT(CASE WHEN event_type = 'transaction' THEN 1 END) as transacciones,
    SUM(CASE WHEN event_type = 'transaction' THEN amount ELSE 0 END) as gasto_total
FROM realtime_events 
WHERE customer_id IS NOT NULL
GROUP BY customer_id, customer_name
ORDER BY gasto_total DESC;
```

### **ğŸ›ï¸ Performance de Productos**
```sql
SELECT 
    product_id,
    product_name,
    category,
    COUNT(*) as veces_vendido,
    SUM(amount) as ingresos_generados,
    AVG(amount) as precio_promedio
FROM realtime_events 
WHERE event_type = 'transaction_item'
GROUP BY product_id, product_name, category
ORDER BY ingresos_generados DESC;
```

### **ğŸ“ˆ AnÃ¡lisis de Sesiones**
```sql
SELECT 
    outcome,
    COUNT(*) as total_sesiones,
    AVG(EXTRACT(EPOCH FROM (timestamp - (session_data->>'start_time')::timestamp))) as duracion_promedio
FROM customer_sessions 
GROUP BY outcome;
```

## ğŸ¯ CaracterÃ­sticas del DWH

### **âœ… Ventajas del DiseÃ±o**
- **Flexibilidad**: JSONB permite almacenar estructuras variables
- **Trazabilidad**: `processed_at` y `source` para auditorÃ­a
- **Escalabilidad**: Ãndices en campos clave
- **IntegraciÃ³n**: Compatible con herramientas de BI

### **ğŸ”§ Optimizaciones**
- **Ãndices**: En `timestamp`, `event_type`, `customer_id`, `product_id`
- **Particionado**: Por fecha para consultas histÃ³ricas
- **CompresiÃ³n**: JSONB optimizado para PostgreSQL
- **RetenciÃ³n**: PolÃ­tica de limpieza de datos antiguos

### **ğŸ“Š IntegraciÃ³n con Dashboard**
- **Redis**: MÃ©tricas en tiempo real desde Spark
- **DWH**: Datos histÃ³ricos para anÃ¡lisis
- **Streamlit**: VisualizaciÃ³n combinada de ambas fuentes

## ğŸš€ Dual Pipeline CDC - Uso en el Sistema

### **ğŸ”„ Flujo Completo CDC**

```mermaid
graph TB
    A[PostgreSQL] --> B[Debezium CDC]
    B --> C[Kafka Topics]
    C --> D[Python Consumer]
    C --> E[Spark Streaming]
    D --> F[DWH PostgreSQL]
    E --> G[Redis Cache]
    F --> H[Dashboard HistÃ³rico]
    G --> I[Dashboard Tiempo Real]
    H --> J[Streamlit Dashboard]
    I --> J
```

### **ğŸ“Š Pipeline HistÃ³rico (DWH)**
1. **CDC Consumer** â†’ Inserta eventos individuales en `RealtimeEvents`
2. **Datos Preservados** â†’ Todos los eventos CDC almacenados
3. **AnÃ¡lisis HistÃ³rico** â†’ Consultas SQL para tendencias y patrones
4. **Reportes** â†’ MÃ©tricas a largo plazo y anÃ¡lisis temporal

### **âš¡ Pipeline Tiempo Real (Redis)**
1. **Spark Streaming** â†’ Procesa eventos CDC en tiempo real
2. **MÃ©tricas Calculadas** â†’ Agregaciones y KPIs en Redis
3. **Dashboard Interactivo** â†’ Visualizaciones actualizadas al instante
4. **Latencia Sub-segundo** â†’ Respuesta inmediata a cambios

### **ğŸ¯ Beneficios del Dual Pipeline**
- **ğŸ“Š Tiempo Real**: Dashboard responsivo con mÃ©tricas actualizadas
- **ğŸ—„ï¸ HistÃ³rico**: Datos preservados para anÃ¡lisis profundo
- **âš¡ Escalabilidad**: SeparaciÃ³n de responsabilidades
- **ğŸ”„ Resiliencia**: Fallback entre sistemas
- **ğŸ¨ Flexibilidad**: Diferentes latencias para diferentes necesidades

Â¡El DWH estÃ¡ optimizado para anÃ¡lisis en tiempo real e histÃ³rico con CDC! ğŸ‰
