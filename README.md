# ðŸ›’ InstaShop Analytics Platform

Plataforma completa de anÃ¡lisis de datos para comercio electrÃ³nico usando **Change Data Capture (CDC)**, **Apache Kafka**, **Apache Spark** y **Redis** con dashboard en tiempo real.

## ðŸ—ï¸ Arquitectura CDC + Dual Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚    â”‚   Debezium     â”‚    â”‚   Apache Kafka  â”‚
â”‚   (5 bases)     â”‚â”€â”€â”€â–¶â”‚   CDC Engine   â”‚â”€â”€â”€â–¶â”‚   (3 brokers)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â”‚                        â”‚                        â”‚
         â”‚                        â”‚                        â”‚
         â”‚                        â”‚                        â–¼
         â”‚                        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                        â”‚              â”‚   Apache Spark â”‚
         â”‚                        â”‚              â”‚   Streaming    â”‚
         â”‚                        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â”‚                        â”‚                        â–¼
         â”‚                        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                        â”‚              â”‚     Redis      â”‚
         â”‚                        â”‚              â”‚   (Cache)      â”‚
         â”‚                        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â”‚                        â”‚                        â–¼
         â”‚                        â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                        â”‚              â”‚ ðŸŽ¯ Streamlit   â”‚
         â”‚                        â”‚              â”‚   Dashboard    â”‚
         â”‚                        â”‚              â”‚ ðŸ“Š Real-time   â”‚
         â”‚                        â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â”‚                        â–¼
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚   Python        â”‚
         â”‚              â”‚   Consumer      â”‚
         â”‚              â”‚   (CDC â†’ DWH)   â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data          â”‚    â”‚   Data          â”‚
â”‚   Warehouse     â”‚    â”‚   Warehouse     â”‚
â”‚   (DWH)         â”‚    â”‚   (DWH)         â”‚
â”‚   Historical    â”‚    â”‚   Real-time     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ”„ Dos Flujos Principales

#### **1. ðŸ“Š CDC â†’ Spark â†’ Redis â†’ Dashboard (Tiempo Real)**
```
PostgreSQL â†’ Debezium â†’ Kafka â†’ Spark Streaming â†’ Redis â†’ Streamlit Dashboard
```
- **PropÃ³sito**: MÃ©tricas en tiempo real para dashboards
- **Latencia**: Sub-segundo
- **Datos**: Agregaciones y mÃ©tricas calculadas

#### **2. ðŸ—„ï¸ CDC â†’ Python Consumer â†’ Data Warehouse (HistÃ³rico)**
```
PostgreSQL â†’ Debezium â†’ Kafka â†’ Python Consumer â†’ DWH PostgreSQL
```
- **PropÃ³sito**: Almacenamiento histÃ³rico para anÃ¡lisis
- **Latencia**: Cerca de tiempo real
- **Datos**: Eventos individuales preservados

### ðŸ—„ï¸ Bases de Datos (PostgreSQL)
- **instashop** (Puerto 5432): Base principal con transacciones, customers y productos
- **crm_db** (Puerto 5433): Sistema CRM con datos de clientes
- **erp_db** (Puerto 5434): Sistema ERP con inventario y stock
- **ecommerce_db** (Puerto 5435): Datos de e-commerce
- **dwh_db** (Puerto 5436): Data Warehouse para anÃ¡lisis

### âš¡ Streaming (Apache Kafka + Debezium CDC)
- **kafka1** (Puerto 9092): Broker principal
- **kafka2** (Puerto 9093): Broker secundario  
- **kafka3** (Puerto 9094): Broker terciario
- **debezium-connect** (Puerto 8083): CDC Engine para PostgreSQL

### ðŸ”¥ Big Data (Apache Spark + Redis)
- **Spark Master** (Puerto 8080): Interfaz web de Spark
- **Redis** (Puerto 6379): Cache en tiempo real para mÃ©tricas
- **Jupyter Notebook** (Puerto 8888): AnÃ¡lisis interactivo

## ðŸš€ Inicio RÃ¡pido

### 1. Levantar la Infraestructura
```bash
docker compose up -d
```

### 2. Configurar CDC (Change Data Capture)
```bash
# Activar entorno virtual
source venv/bin/activate

# Configurar PostgreSQL para CDC
python nico-scripts/init_postgres_cdc.py

# Desplegar conectores Debezium
python nico-scripts/setup_debezium.py
```

### 3. Poblar con Datos de Prueba
```bash
# Generar datos iniciales
python fake-data.py

# Generar datos realistas en tiempo real
python nico-scripts/realistic_data_generator.py
```

### 4. Ejecutar Dual Pipeline CDC
```bash
# Terminal 1: CDC â†’ Data Warehouse (HistÃ³rico)
python nico-scripts/cdc_dwh_consumer.py

# Terminal 2: CDC â†’ Spark â†’ Redis (Tiempo Real)
python nico-scripts/spark-streaming/cdc_spark_redis.py
```

### 5. Lanzar Dashboard
```bash
streamlit run nico-scripts/dashboards/realtime_spark_dashboard.py --server.port 8501
```

**ðŸŒ Dashboard disponible en:** http://localhost:8501

## ðŸ”„ Dual Pipeline CDC

### ðŸ“Š Pipeline Tiempo Real (Spark + Redis)
**PropÃ³sito**: Dashboard interactivo con mÃ©tricas en tiempo real

```bash
# Ejecutar Spark Streaming
python nico-scripts/spark-streaming/cdc_spark_redis.py
```

**MÃ©tricas generadas en Redis:**
- `cdc_spark_metrics:transactions` - Transacciones, revenue, clientes Ãºnicos
- `cdc_spark_metrics:products` - Ventas por categorÃ­a
- `metrics:behavior:*` - Eventos de comportamiento (bÃºsquedas, vistas, carrito)

### ðŸ—„ï¸ Pipeline HistÃ³rico (DWH)
**PropÃ³sito**: Almacenamiento histÃ³rico para anÃ¡lisis y reportes

```bash
# Ejecutar Consumer CDC â†’ DWH
python nico-scripts/cdc_dwh_consumer.py
```

**Datos almacenados en DWH:**
- `realtime_events` - Todos los eventos CDC preservados
- Eventos de transacciones, comportamiento, productos
- Timestamps precisos para anÃ¡lisis temporal

### ðŸŽ¯ Beneficios del Dual Pipeline
- **Tiempo Real**: Dashboard responsivo con mÃ©tricas actualizadas
- **HistÃ³rico**: AnÃ¡lisis de tendencias y patrones a largo plazo
- **Escalabilidad**: SeparaciÃ³n de responsabilidades
- **Resiliencia**: Fallback entre sistemas

## ðŸ“Š KPIs y MÃ©tricas

### ðŸ’° MÃ©tricas Financieras
- **Revenue Total**: Ingresos acumulados
- **AOV (Average Order Value)**: Valor promedio por pedido
- **Revenue 30d**: Ingresos Ãºltimos 30 dÃ­as
- **Growth Rate**: Tasa de crecimiento

### ðŸ‘¥ MÃ©tricas de Clientes
- **Customers Activos**: Clientes con transacciones
- **SegmentaciÃ³n por Plan**: Basic, Premium, Enterprise
- **Customer Lifetime Value**: Valor de vida del cliente
- **Retention Rate**: Tasa de retenciÃ³n

### ðŸ“¦ MÃ©tricas de Inventario
- **Stock Status**: CrÃ­tico, Bajo, Ã“ptimo
- **Productos CrÃ­ticos**: Stock por debajo del punto de reorden
- **RotaciÃ³n de Inventario**: Velocidad de movimiento
- **Almacenes**: DistribuciÃ³n por ubicaciÃ³n

### ðŸ›ï¸ MÃ©tricas de Ventas
- **Transacciones Diarias**: Volumen de operaciones
- **CategorÃ­as Top**: Productos mÃ¡s vendidos
- **MÃ©todos de Pago**: DistribuciÃ³n de pagos
- **Tendencias**: AnÃ¡lisis temporal

## ðŸ“ Estructura del Proyecto

```
instashop/
â”œâ”€â”€ ðŸ³ docker-compose.yml                    # OrquestaciÃ³n de servicios
â”œâ”€â”€ ðŸ“Š fake-data.py                         # Generador de datos iniciales
â”œâ”€â”€ ðŸ“ debezium-config/                     # Configuraciones CDC
â”‚   â”œâ”€â”€ instashop-connector.json            # Conector principal
â”‚   â”œâ”€â”€ crm-connector.json                  # Conector CRM
â”‚   â”œâ”€â”€ erp-connector.json                  # Conector ERP
â”‚   â””â”€â”€ ecommerce-connector.json            # Conector E-commerce
â”œâ”€â”€ ðŸ“ nico-scripts/                        # Scripts principales
â”‚   â”œâ”€â”€ init_postgres_cdc.py               # ConfiguraciÃ³n CDC PostgreSQL
â”‚   â”œâ”€â”€ setup_debezium.py                 # Despliegue conectores CDC
â”‚   â”œâ”€â”€ realistic_data_generator.py        # Generador datos realistas
â”‚   â”œâ”€â”€ cdc_dwh_consumer.py                # Consumer CDC â†’ DWH
â”‚   â”œâ”€â”€ ðŸ“ spark-streaming/                # Scripts Spark
â”‚   â”‚   â””â”€â”€ cdc_spark_redis.py            # Spark CDC â†’ Redis
â”‚   â””â”€â”€ ðŸ“ dashboards/                     # Dashboards
â”‚       â””â”€â”€ realtime_spark_dashboard.py    # Dashboard principal
â”œâ”€â”€ ðŸ“ data/                               # Datos persistentes PostgreSQL
â”œâ”€â”€ ðŸ venv/                               # Entorno virtual Python
â””â”€â”€ ðŸ“‹ README.md
```

## ðŸ› ï¸ TecnologÃ­as

### Backend
- **ðŸ³ Docker & Docker Compose**: OrquestaciÃ³n de contenedores
- **ðŸ—„ï¸ PostgreSQL 15**: Base de datos relacional
- **âš¡ Apache Kafka**: Streaming de datos
- **ðŸ”¥ Apache Spark**: Procesamiento Big Data
- **ðŸ Python 3.12**: Lenguaje principal

### Frontend & Analytics
- **ðŸŽ¯ Streamlit**: Dashboard interactivo
- **ðŸ“Š Plotly**: Visualizaciones dinÃ¡micas
- **ðŸ¼ Pandas**: ManipulaciÃ³n de datos
- **ðŸ““ Jupyter**: AnÃ¡lisis exploratorio

### LibrerÃ­as Python
```txt
streamlit==1.28.0
plotly==5.17.0
pandas==2.1.0
psycopg2-binary==2.9.7
faker==19.6.0
pyspark==3.4.0
```

## ðŸ”§ ConfiguraciÃ³n Detallada

### Variables de Entorno
```env
# PostgreSQL
POSTGRES_USER=insta
POSTGRES_PASSWORD=insta123
POSTGRES_DB=instashop

# Kafka
KAFKA_CLUSTER_ID=instashop-cluster-1
KAFKA_BROKERS=kafka1:9092,kafka2:9093,kafka3:9094

# Spark
SPARK_MASTER_URL=spark://spark:7077
```

### Puertos Utilizados
| Servicio | Puerto | DescripciÃ³n |
|----------|--------|-------------|
| PostgreSQL Main | 5432 | Base principal |
| CRM Database | 5433 | Sistema CRM |
| ERP Database | 5434 | Sistema ERP |
| E-commerce DB | 5435 | E-commerce |
| Data Warehouse | 5436 | DWH Analytics |
| Kafka Broker 1 | 9092 | Streaming |
| Kafka Broker 2 | 9093 | Streaming |
| Kafka Broker 3 | 9094 | Streaming |
| Spark Master | 8080 | Web UI |
| Jupyter | 8888 | Notebooks |
| **Dashboard** | **8501** | **Streamlit App** |

## ðŸ“ˆ Casos de Uso

### ðŸŽ¯ Business Intelligence
- **Dashboards Ejecutivos**: KPIs en tiempo real
- **AnÃ¡lisis de Ventas**: Tendencias y patrones
- **SegmentaciÃ³n de Clientes**: Perfiles y comportamientos
- **OptimizaciÃ³n de Inventario**: GestiÃ³n de stock

### ðŸ” AnÃ¡lisis Avanzado
- **PredicciÃ³n de Demanda**: Machine Learning
- **DetecciÃ³n de AnomalÃ­as**: Transacciones sospechosas
- **Recomendaciones**: Motor de productos
- **AnÃ¡lisis de Sentimiento**: Feedback de clientes

### ðŸš€ Escalabilidad
- **Multi-tenant**: MÃºltiples clientes
- **Microservicios**: Arquitectura distribuida
- **Real-time Processing**: Datos en tiempo real
- **Cloud Ready**: Preparado para la nube

## ðŸ”„ Pipeline ETL

### ExtracciÃ³n
```python
# Datos de mÃºltiples fuentes
- Transacciones (PostgreSQL)
- Clientes CRM (PostgreSQL)
- Inventario ERP (PostgreSQL)
- Eventos Kafka (Streaming)
```

### TransformaciÃ³n
```python
# CÃ¡lculos y agregaciones
- KPIs financieros
- MÃ©tricas de customer
- Estado de inventario
- AnÃ¡lisis temporal
```

### Carga
```python
# Data Warehouse
- Tablas dimensionales
- Hechos agregados
- MÃ©tricas histÃ³ricas
- Cache del dashboard
```

## ðŸŽ¨ Features del Dashboard

### ðŸ“Š Vista General
- **KPIs Principales**: MÃ©tricas clave en tiempo real
- **Filtros Temporales**: AnÃ¡lisis por perÃ­odo
- **ActualizaciÃ³n AutomÃ¡tica**: Cache de 1 minuto
- **Responsive Design**: Adaptable a dispositivos

### ðŸ“ˆ AnÃ¡lisis de Ventas
- **GrÃ¡fico de Tendencias**: Ventas diarias
- **Top CategorÃ­as**: Productos mÃ¡s vendidos
- **MÃ©todos de Pago**: DistribuciÃ³n de pagos
- **AnÃ¡lisis GeogrÃ¡fico**: Ventas por regiÃ³n

### ðŸ‘¥ GestiÃ³n de Clientes
- **SegmentaciÃ³n**: Por plan de suscripciÃ³n
- **Top Customers**: Mayores compradores
- **AnÃ¡lisis de Cohortes**: RetenciÃ³n temporal
- **Customer Journey**: Ruta del cliente

### ðŸ“¦ Control de Inventario
- **Estado del Stock**: CrÃ­tico, Bajo, Ã“ptimo
- **Alertas**: Productos con stock crÃ­tico
- **RotaciÃ³n**: Velocidad de inventario
- **PredicciÃ³n**: Demanda futura

## ðŸš¨ Monitoreo y Alertas

### ðŸ” Health Checks
```bash
# Verificar servicios
docker compose ps

# Logs de servicios
docker compose logs [servicio]

# MÃ©tricas de base de datos
docker compose exec postgres psql -U insta -d instashop -c "SELECT COUNT(*) FROM transaction;"
```

### ðŸ“§ Alertas AutomÃ¡ticas
- **Stock CrÃ­tico**: Productos por debajo del mÃ­nimo
- **Transacciones Fallidas**: Errores de pago
- **Performance**: Consultas lentas
- **Capacidad**: Uso de recursos

## ðŸ”’ Seguridad

### ðŸ” AutenticaciÃ³n
- Usuarios y contraseÃ±as por base de datos
- Conexiones SSL/TLS habilitadas
- Tokens de API para servicios

### ðŸ›¡ï¸ AutorizaciÃ³n
- Roles por servicio
- Permisos granulares
- AuditorÃ­a de accesos

## ðŸ“š Comandos Ãštiles

### ðŸ³ Docker
```bash
# Iniciar servicios
docker compose up -d

# Ver logs
docker compose logs -f

# Reiniciar servicio especÃ­fico
docker compose restart [servicio]

# Limpiar datos
docker compose down -v
```

### ðŸ—„ï¸ Base de Datos
```bash
# Conectar a PostgreSQL
docker compose exec postgres psql -U insta -d instashop

# Backup
docker compose exec postgres pg_dump -U insta instashop > backup.sql

# Restore
docker compose exec postgres psql -U insta -d instashop < backup.sql
```

### ðŸ Python
```bash
# Instalar dependencias
pip install -r requirements.txt

# Ejecutar ETL
python local_etl.py

# Lanzar dashboard
streamlit run dashboard.py

python fake-data.py
```

## ðŸŽ¯ PrÃ³ximos Pasos

### ðŸ“ˆ Mejoras Planificadas

- [ ] **Machine Learning**: Modelos predictivos
- [ ] **API REST**: Endpoints para integraciÃ³n
- [ ] **Notificaciones**: Email/Slack alerts
- [ ] **ExportaciÃ³n**: PDF/Excel reports
- [ ] **Multi-idioma**: Soporte i18n

### ðŸš€ Escalabilidad

- [ ] **Kubernetes**: OrquestaciÃ³n avanzada
- [ ] **Redis Cache**: Cache distribuido
- [ ] **Load Balancer**: Alta disponibilidad
- [ ] **Monitoring**: Prometheus + Grafana

## ðŸ“ž Soporte

Para problemas o mejoras:

1. **ðŸ” Verificar logs**: `docker compose logs`
2. **ðŸ”„ Reiniciar servicios**: `docker compose restart`
3. **ðŸ“§ Reportar issues**: Crear ticket con logs

---

**ðŸ“Š InstaShop Analytics Platform** - Transformando datos en decisiones de negocio

Desarrollado con â¤ï¸ usando Docker, Python y Streamlit
