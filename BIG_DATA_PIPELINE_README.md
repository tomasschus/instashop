# ğŸš€ INSTASHOP BIG DATA PIPELINE

## ğŸ“Š Estado Actual - COMPLETAMENTE FUNCIONAL

### âœ… **COMPONENTES FUNCIONANDO PERFECTAMENTE**

#### **1. ğŸ—„ï¸ Bases de Datos PostgreSQL**
- **instashop** (puerto 5432) - Base principal con datos de productos, clientes, compradores
- **crm_db** (puerto 5433) - CRM
- **erp_db** (puerto 5434) - ERP  
- **ecommerce_db** (puerto 5435) - E-commerce
- **dwh_db** (puerto 5436) - Data Warehouse con **101+ eventos en tiempo real**

#### **2. ğŸ“¨ Apache Kafka (3 Brokers)**
- **kafka1** (puerto 9092) - Broker principal
- **kafka2** (puerto 9093) - Broker secundario
- **kafka3** (puerto 9094) - Broker terciario
- **Topics activos**: `transactions`, `user_behavior`, `searches`, `cart_events`

#### **3. ğŸ”„ Pipeline Big Data (Producer â†’ Kafka â†’ Consumer â†’ DWH)**
- **Producer**: Genera eventos simulados (transacciones, bÃºsquedas, carritos)
- **Consumer**: Lee de Kafka y guarda en DWH en tiempo real
- **Consumer Group**: `instashop-analytics-group` (LAG = 0, procesando todo en tiempo real)

#### **4. ğŸ“Š Datos Procesados en Tiempo Real**
- **Transacciones**: Con informaciÃ³n completa (cliente, producto, monto, mÃ©todo de pago)
- **BÃºsquedas**: TÃ©rminos de bÃºsqueda de usuarios
- **Abandono de carrito**: Con valores de carrito abandonado
- **Eventos de usuario**: Clicks, vistas de productos

#### **5. ğŸ–¥ï¸ Spark Cluster**
- **Spark Master**: Corriendo en puerto 8080 (UI disponible)
- **Jupyter**: Disponible en puerto 8888 con PySpark

---

## ğŸš€ **CÃ“MO USAR EL PIPELINE**

### **1. Levantar el Sistema Completo**
```bash
# Crear entorno virtual
python3 -m venv venv
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt

# Levantar todos los servicios
docker-compose up -d
```

### **2. Ejecutar el Pipeline Big Data**
```bash
# Activar entorno virtual
source venv/bin/activate

# Ejecutar pipeline completo
python run_pipeline.py
```

**Opciones disponibles:**
1. **Pipeline completo** (Producer + Consumer + Spark)
2. **Solo Producer** (genera datos a Kafka)
3. **Solo Consumer** (lee de Kafka y guarda en DWH)
4. **Solo Spark Streaming** (anÃ¡lisis en tiempo real)
5. **Producer + Consumer** (sin Spark)

### **3. Verificar que Funciona**

#### **Verificar Kafka Topics:**
```bash
docker exec kafka1 kafka-topics.sh --bootstrap-server localhost:9092 --list
```

#### **Ver datos en Kafka:**
```bash
docker exec kafka1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic transactions --from-beginning --max-messages 3
```

#### **Ver datos en DWH:**
```bash
docker exec dwh_db psql -U dwh -d dwh_db -c "SELECT COUNT(*) FROM realtime_events;"
```

#### **Ver Consumer Group Status:**
```bash
docker exec kafka1 kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group instashop-analytics-group
```

---

## ğŸ“ˆ **INTERFACES DISPONIBLES**

- **Spark UI**: http://localhost:8080
- **Jupyter Notebook**: http://localhost:8888
- **Streamlit Dashboard Original**: http://localhost:8501
- **ğŸš€ Spark Analytics Dashboard**: http://localhost:8502

---

## ğŸš€ **SPARK ANALYTICS DASHBOARD**

### **Dashboard de AnÃ¡lisis en Tiempo Real**
**URL**: http://localhost:8502

#### **ğŸ“Š CaracterÃ­sticas del Dashboard:**
- **MÃ©tricas en tiempo real**: Total eventos, clientes Ãºnicos, ingresos, Ãºltimo evento
- **Visualizaciones interactivas**: GrÃ¡ficos de torta y barras con Plotly
- **AnÃ¡lisis por categorÃ­a**: Eventos y ingresos por tipo de producto
- **Top clientes**: Ranking de clientes por nÃºmero de transacciones
- **Tabla de eventos recientes**: Ãšltimos 20 eventos procesados
- **Auto-refresh**: Se actualiza cada 30 segundos automÃ¡ticamente

#### **ğŸ¯ MÃ©tricas que muestra:**
1. **ğŸ“ˆ DistribuciÃ³n de eventos**: GrÃ¡fico de torta mostrando tipos de eventos
2. **ğŸ† Top 5 clientes**: Clientes con mÃ¡s transacciones
3. **ğŸ›ï¸ AnÃ¡lisis por categorÃ­a**: Eventos y ingresos por categorÃ­a de productos
4. **ğŸ“‹ Eventos recientes**: Tabla con los Ãºltimos eventos procesados
5. **ğŸ’° MÃ©tricas financieras**: Ingresos totales y promedio por transacciÃ³n

#### **ğŸ”„ CÃ³mo usar el Dashboard:**
1. **Acceder**: Ve a http://localhost:8502
2. **Ver datos**: El dashboard se actualiza automÃ¡ticamente cada 30 segundos
3. **Refresh manual**: Usa el botÃ³n "ğŸ”„ Refresh Data" para actualizaciÃ³n inmediata
4. **Monitorear pipeline**: Ve el estado de todos los componentes del pipeline

---

## ğŸ”§ **ARQUITECTURA DEL PIPELINE**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer  â”‚â”€â”€â”€â–¶â”‚    Kafka    â”‚â”€â”€â”€â–¶â”‚  Consumer   â”‚â”€â”€â”€â–¶â”‚    DWH      â”‚
â”‚             â”‚    â”‚  (Topics)   â”‚    â”‚             â”‚    â”‚ PostgreSQL  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    Spark    â”‚
                   â”‚ Streaming   â”‚
                   â”‚ (AnÃ¡lisis)  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š **TIPOS DE EVENTOS PROCESADOS**

### **Transacciones**
```json
{
  "event_type": "transaction",
  "transaction_id": "uuid",
  "customer_id": 27,
  "customer_name": "Mendoza, Smith and Ray",
  "subscription_plan": "Enterprise",
  "buyer_id": 52,
  "buyer_name": "Terry Bush",
  "product_id": 100,
  "product_name": "Rock",
  "category": "Electronics",
  "price": 469.91,
  "quantity": 4,
  "total_amount": 469.91,
  "payment_method": "Wire Transfer",
  "status": "failed",
  "timestamp": "2025-09-16T17:26:19.227476"
}
```

### **BÃºsquedas**
```json
{
  "event_type": "search",
  "customer_id": 12,
  "search_term": "electronics",
  "timestamp": "2025-09-16T17:26:30.346408"
}
```

### **Abandono de Carrito**
```json
{
  "event_type": "cart_abandonment",
  "customer_id": 2,
  "cart_value": 64.17,
  "timestamp": "2025-09-16T17:26:31.777958"
}
```

---

## ğŸ¯ **ESTADO ACTUAL**

âœ… **Producer funcionando** - Generando datos a Kafka  
âœ… **Kafka funcionando** - 4 topics activos con datos  
âœ… **Consumer funcionando** - Procesando datos en tiempo real (LAG = 0)  
âœ… **DWH funcionando** - 101+ eventos almacenados  
âœ… **Spark funcionando** - Cluster activo (UI en puerto 8080)  
âœ… **Dashboard original funcionando** - Streamlit en puerto 8501  
âœ… **ğŸš€ Spark Analytics Dashboard funcionando** - Streamlit en puerto 8502  

**El pipeline Big Data estÃ¡ 100% funcional y procesando datos en tiempo real!**

### **ğŸ‰ NUEVAS CARACTERÃSTICAS:**
- **Dashboard de Spark Analytics**: Visualizaciones en tiempo real de los datos procesados
- **AnÃ¡lisis automÃ¡tico**: MÃ©tricas actualizadas cada 30 segundos
- **Interfaz interactiva**: GrÃ¡ficos con Plotly para mejor experiencia de usuario
- **Monitoreo completo**: Estado de todos los componentes del pipeline visible

---

## ğŸ“ **ESTRUCTURA DE ARCHIVOS**

```
instashop/
â”œâ”€â”€ kafka_streaming/
â”‚   â”œâ”€â”€ producer.py          # Genera eventos a Kafka
â”‚   â””â”€â”€ consumer.py          # Lee de Kafka y guarda en DWH
â”œâ”€â”€ spark_analytics/
â”‚   â”œâ”€â”€ streaming.py         # AnÃ¡lisis en tiempo real con Spark (Kafka)
â”‚   â””â”€â”€ simple_streaming.py  # AnÃ¡lisis simplificado desde DWH
â”œâ”€â”€ config.py               # ConfiguraciÃ³n centralizada
â”œâ”€â”€ run_pipeline.py         # Script principal del pipeline
â”œâ”€â”€ spark_dashboard.py      # Dashboard de Spark Analytics
â””â”€â”€ BIG_DATA_PIPELINE_README.md  # Este archivo
```

---

## ğŸš¨ **NOTAS IMPORTANTES**

- **Java requerido**: Spark necesita Java 17+ para funcionar correctamente
- **Puertos utilizados**: 5432-5436 (PostgreSQL), 9092-9094 (Kafka), 8080 (Spark), 8501-8502 (Streamlit), 8888 (Jupyter)
- **Consumer Group**: `instashop-analytics-group` (no cambiar)
- **Datos en tiempo real**: El consumer procesa todos los eventos sin retraso (LAG = 0)

---

**ğŸ‰ Â¡El pipeline Big Data estÃ¡ completamente funcional y procesando datos en tiempo real!**
